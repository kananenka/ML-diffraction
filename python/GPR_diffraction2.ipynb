{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression model for analyzing diffraction data\n",
    "\n",
    "* Input: diffraction patterns taken from \"xdr\" files \n",
    "* Output: composition profiles taken from \"xy\" files\n",
    "\n",
    "### GPR:\n",
    "\n",
    "Composition profile at each point (distance) $d_i$ is approximated by:\n",
    "$$ C_i(\\tilde{D}) = \\sum_j \\omega_j^i k_\\sigma(\\tilde{D},D)$$\n",
    "where $D$ and $\\tilde{D}$ are two diffraction patterns, $K(\\tilde{D},D)$ is the kernel which measures\n",
    "the similarity between $D$ and $\\tilde{D}$, $\\{\\omega\\}$ is the set of weights to\n",
    "be optimized, and $i$ is the distance grid point index: $d_i = i\\Delta d$ ($\\Delta d$ is the\n",
    "spacing between gridpoints).\n",
    "\n",
    "Optimized weights are given by:\n",
    "\n",
    "$$ \\boldsymbol{\\omega}^i = \\left( \\mathbf{K}_\\sigma + \\alpha \\mathbf{I}\\right)^{-1} \\mathbf{C}_i$$\n",
    "\n",
    "where $\\boldsymbol{\\omega}^i$ and $\\mathbf{C}_i$ are vectors containing weights and known compositions\n",
    "for each training sample, $\\alpha$ is an adjustable\n",
    "parameter, and $\\mathbf{K}_\\sigma$ is the kernel (covariance) matrix. We start with the \n",
    "most commonly used Gaussian kernel function and will try several other kernels as well.\n",
    "\n",
    "* Gaussian (or RBF) kernel: \n",
    "$$K_\\sigma(D^i,D^j) = \\exp \\left(-\\frac{\\sum_k || D^i_k - D^j_k||^2}{2\\sigma^2}\\right)$$\n",
    "where $|| \\cdots ||$ is some distance metric and $\\sigma$ controls the width of a Gaussian \n",
    "and is another adjustable parameter. \n",
    "Since $D$ is a 1D array of intensities we \n",
    "take $|| \\cdots ||$ to be just the difference between\n",
    "two points of $D^i$ and $D^j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas  = [1e-7] #np.logspace(-9,-4,6,True) \n",
    "sigmas  = [10.0] #np.logspace(-4,1,6,True)  \n",
    "\n",
    "errors = np.zeros((len(sigmas), len(alphas)))\n",
    "\n",
    "for l, alpha in enumerate(alphas):\n",
    "    for s, sigma in enumerate(sigmas):\n",
    "        errors[s,l] = run(sigma, alpha)\n",
    "        #print (\" sigma = \",sigma,\" alpha = \",alpha,\" error = \",errors[s,l])\n",
    "        \n",
    "plot_data(alphas, sigmas, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (RBF, Matern)\n",
    "\n",
    "def run(sigma, alpha):\n",
    "    \"\"\"\n",
    "       Main routine:\n",
    "       since we don't have a lot of data we perform k-fold cross validation\n",
    "       and average errors of each run\n",
    "    \n",
    "    \"\"\"\n",
    "    Ndata  = 102\n",
    "    Ntrain = 92\n",
    "    Ntest  = Ndata - Ntrain\n",
    "    n_cross_valid = 3\n",
    "    \n",
    "    cv_errors = np.zeros((n_cross_valid))\n",
    "    errors = np.zeros((Ntest))\n",
    "    \n",
    "    qgrid, ints, xgrid_raw, comp_raw, xgrid, comp = load_data() \n",
    "    \n",
    "    inds = np.arange(Ndata)\n",
    "    np.random.shuffle(inds)   \n",
    "    \n",
    "    kernel = RBF(length_scale=sigma, length_scale_bounds=(1e-5, 1e5))\n",
    "    #kernel = Matern(length_scale=sigma, length_scale_bounds=(1e-3, 1e3), nu=2.5)\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha, n_restarts_optimizer=20)\n",
    "    \n",
    "    for k in range(n_cross_valid):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(ints, comp, Ntrain, Ntest, k, n_cross_valid, inds)\n",
    "    \n",
    "        gp.fit(x_train, y_train)\n",
    "        y_pred, sigma = gp.predict(x_test, return_std=True)\n",
    "        \n",
    "        errors[:] = 0.0\n",
    "        for m in range(Ntest): \n",
    "            errors[m] = np.sqrt(np.mean((y_pred[m,:]-y_test[m,:])**2))\n",
    "            plot_compare(xgrid, y_pred[m,:], xgrid, y_test[m,:], \"GPR predicted\", \"Reference\", m, k)\n",
    "        \n",
    "        cv_errors[k] = np.mean(errors)\n",
    "    \n",
    "    return np.mean(cv_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data here\n",
    "\n",
    "* We focus on a range of Q=[25,29] and Q=[51,58] where the two sets of peaks are located.\n",
    "This allows us to reduce the size of the input vector to 1100 intensity values.\n",
    "\n",
    "* Scaling factor just scales the intensity (it is probably not needed, just feels better to deal with numbers on the order of 1).\n",
    "\n",
    "\n",
    "## Composition profiles\n",
    "First read each line in each file which is 2750 lines per file for x3 files, 4598 for x5 and, 6446 for x7 files. \n",
    "It sounds like too many so we will set a coarser resolution but make it adjustable. Right now we set the new\n",
    "resolution to be 0.1 instead of native 0.01. Results are good with the new resolution.\n",
    "\n",
    "### TO DO LIST:\n",
    "\n",
    "<input type=\"checkbox\"> Try original resolution of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    datadir = \"../../data/AuPtSL20x20x7-1/\"\n",
    "    ndata = 102\n",
    "    scaling_factor = 1e10\n",
    "    \n",
    "    # Q grid is only needed if we want to plot diffraction\n",
    "    # pattern, it is not used in the GPR model\n",
    "    # take low Q features with Q=[25,29]\n",
    "    # and high Q features with Q=[51,58]\n",
    "    qminL = 25.0\n",
    "    qmaxL = 29.0\n",
    "    qminH = 51.0\n",
    "    qmaxH = 58.0\n",
    "    dq = 0.01\n",
    "    \n",
    "    NQgridL = int((qmaxL - qminL)/dq)\n",
    "    NQgridH = int((qmaxH - qminH)/dq)\n",
    "    NQgrid = NQgridL + NQgridH\n",
    "    \n",
    "    dp = np.zeros((ndata, NQgrid))\n",
    "    qgridL = np.linspace(qminL, qmaxL, NQgridL, False)\n",
    "    qgridH = np.linspace(qminH, qmaxH, NQgridH, False)\n",
    "    \n",
    "    qgrid = np.concatenate((qgridL, qgridH), axis=0)\n",
    "    \n",
    "    case = \"0A\"\n",
    "    conv = \"B\"\n",
    "    # read xrd files\n",
    "    for n in range(ndata):\n",
    "        filename = datadir+\"AuPtSLxrd\"+case+\"20x20x7-\"+conv+\"/AuPtSLxrd\"+case+\"20x20x7-\"+str(n)+conv\n",
    "        f = open(filename,\"r\")\n",
    "        m = 0\n",
    "        for line in f:\n",
    "            line2 = line.split()\n",
    "            qval = float(line2[0])\n",
    "            ints = float(line2[1])\n",
    "            if qval >= qminL and qval < qmaxL:\n",
    "                dp[n, m] = ints\n",
    "                m += 1\n",
    "            if qval >= qminH and qval < qmaxH:\n",
    "                dp[n, m] = ints\n",
    "                m += 1\n",
    "        f.close()\n",
    "    \n",
    "    dp[:,:] /= scaling_factor\n",
    "    \n",
    "    # reading xy files:\n",
    "    #\n",
    "    # Number of lines to read for each case:\n",
    "    # ---------------------------------------\n",
    "    # 0A - 6446\n",
    "    # 2A - 6492\n",
    "    # 2B - 6424\n",
    "    #\n",
    "    nxy_grid_raw = 6446\n",
    "    dxy = 0.01\n",
    "    xygrid_raw = np.linspace(0,nxy_grid_raw*dxy, nxy_grid_raw, False)\n",
    "    \n",
    "    xy_raw = np.zeros((ndata, nxy_grid_raw))\n",
    "    \n",
    "    for n in range(ndata):\n",
    "        filename = datadir + \"AuPtSLxy\"+case+\"20x20x7/AuPtSLxy\"+case+\"20x20x7-\"+str(n)\n",
    "        f = open(filename,\"r\")\n",
    "        # skip first 7 lines\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        f.readline()\n",
    "        m = 0\n",
    "        for line in f:\n",
    "            line2 = line.split()\n",
    "            xyval = float(line2[1])\n",
    "            xy_raw[n, m] = xyval\n",
    "            m += 1\n",
    "        f.close()\n",
    "        \n",
    "    # apply coarser resolution here\n",
    "    # new resolution will average over a set of points and take an average\n",
    "    # x point new grid point\n",
    "    dN = 10\n",
    "    Nxygrid = int(nxy_grid_raw/dN)\n",
    "    \n",
    "    xygrid = np.zeros((Nxygrid))\n",
    "    xy = np.zeros((ndata, Nxygrid))\n",
    "    \n",
    "    for m in range(ndata):\n",
    "        for n in range(Nxygrid):\n",
    "            xygrid[n] = np.mean(xygrid_raw[n*dN:n*dN+dN])\n",
    "            xy[m,n] = np.mean(xy_raw[m,n*dN:n*dN+dN])\n",
    "        \n",
    "    return qgrid, dp, xygrid_raw, xy_raw, xygrid, xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ints, comp, ntrain, ntest, k, n_cross_val, inds):\n",
    "    \"\"\"\n",
    "        Split data into train and test sets\n",
    "        for a step k in n-fold cross validation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ndata = ints.shape[0]\n",
    "    \n",
    "    assert ndata == (ntrain + ntest), \"ndata does not match ntrain + ntest in train_test_split\"\n",
    "    \n",
    "    ind_test  = inds[k*ntest:k*ntest+ntest]\n",
    "    ind_train = np.concatenate((inds[:k*ntest],inds[(k+1)*ntest:]),axis=0)\n",
    "    \n",
    "    x_train = np.zeros((ind_train.shape[0], ints.shape[1]))\n",
    "    x_test  = np.zeros((ind_test.shape[0],  ints.shape[1]))\n",
    "    y_train = np.zeros((ind_train.shape[0], comp.shape[1]))\n",
    "    y_test  = np.zeros((ind_test.shape[0],  comp.shape[1]))\n",
    "    \n",
    "    for n, m in enumerate(ind_train):\n",
    "        x_train[n,:] = ints[m,:]\n",
    "        y_train[n,:] = comp[m,:]\n",
    "        \n",
    "    for n, m in enumerate(ind_test):\n",
    "        x_test[n,:] = ints[m,:]\n",
    "        y_test[n,:] = comp[m,:]\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "def plot_compare(x, y, x1, y1, label1, label2, m, k):\n",
    "    \n",
    "    fontsize=16\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(x, y, color='g', label=label1)\n",
    "    ax.plot(x1, y1, color='r', ls=':', label=label2)\n",
    "    ax.legend(loc='upper right',fontsize=fontsize)\n",
    "    plt.tight_layout()\n",
    "    ax.set_xlabel(\"d\",fontsize=fontsize)\n",
    "    ax.set_ylabel(\"Composition, %\",fontsize=fontsize)\n",
    "    \n",
    "    case = \"0A\"\n",
    "    conv = \"B\"\n",
    "    dirname  = \"../results/AuPtSL20x20x7-1/\"+case+\"-\"+conv+\"/\"\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        \n",
    "    fname = dirname + str(k) + \"_\" + str(m) + \".pdf\"\n",
    "    plt.savefig(fname,dpi=1200,bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x, y):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    ax.plot(x, y, color='g')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(x,y,data):\n",
    "    \"\"\"\n",
    "       Plot errors in grid search\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    fontsize=16\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax  = fig.add_subplot(111)\n",
    "    cax = ax.matshow(100*data, interpolation='nearest', vmin=0, vmax=1.0)\n",
    "    \n",
    "    cbar=fig.colorbar(cax)\n",
    "    cbar.ax.set_ylabel('100*RMSD',rotation=90,fontsize=fontsize)\n",
    "\n",
    "    # put text on matrix elements\n",
    "    for i, x_val in enumerate(np.arange(len(x))):\n",
    "        for j, y_val in enumerate(np.arange(len(y))):\n",
    "            c = \"{0:.3f}\".format(100*data[j,i])  \n",
    "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
    "\n",
    "    # convert axis values to string labels\n",
    "    x=[str(i) for i in x]\n",
    "    y=[str(i) for i in y]\n",
    "\n",
    "    ax.set_xticklabels(['']+x)\n",
    "    ax.set_yticklabels(['']+y)\n",
    "\n",
    "    ax.set_xlabel('$\\\\alpha$',fontsize=fontsize)\n",
    "    ax.set_ylabel('$\\\\sigma$',fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
